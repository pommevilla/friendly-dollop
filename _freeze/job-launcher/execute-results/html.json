{
  "hash": "ad6e89e39d391801445e36453caa154d",
  "result": {
    "markdown": "---\nknit: quarto render\n---\n\n# Job Launcher {.pro-header}\n\n## Overview\n\nThe RStudio Job Launcher provides the ability for RStudio Workbench to start processes within various batch processing systems (e.g., Slurm) and container orchestration platforms (e.g., Kubernetes). RStudio Workbench integrates with the Job Launcher to allow you to run your R Sessions within your compute cluster software of choice, and allows you to containerize your sessions for maximum process isolation and operations efficiency. Furthermore, users can submit standalone adhoc jobs to your compute cluster(s) to run computationally expensive R scripts.\n\n::: {.callout-note}\nIntegration with the Job Launcher is not enabled in all editions of RStudio Workbench. You can run `rstudio-server license-manager status` to see if the Launcher is enabled. If it isn't, contact <sales@rstudio.com> to purchase a license with the Job Launcher enabled.\n:::\n\n## Configuration {#launcher-configuration}\n\n### Job Launcher Configuration\n\nBefore the Job Launcher can be run, it must be properly configured via the config file `/etc/rstudio/launcher.conf`; see the [Job Launcher documentation](https://docs.rstudio.com/job-launcher/99.9.9/index.html) for supported configuration options. If the Launcher was installed with RStudio Workbench, a default working configuration that uses the Local plugin is installed for your convenience.\n\nThe Launcher configuration parameter `admin-group` should be configured to the group value of the RStudio Workbench server user, specified in the `server-user` configuration parameter in `rserver.conf` (which defaults to  rstudio-server). This makes the server user a Job Launcher admin, which is necessary to properly launch sessions on behalf of other users.\n\n### RStudio Workbench Integration\n\nRStudio Workbench must be configured in order to integrate with the Job Launcher. There are several files which house the configuration, and they are described within subsequent sections.\n\n#### Server Configuration\n\nThe RStudio Workbench process `rserver` must be configured to communicate with the Job Launcher in order to enable session launching. The following table lists the various configuration options that are available to be specified in the `rserver.conf` configuration file:\n\n**/etc/rstudio/rserver.conf**\n\nConfig Option | Description | Required (Y/N) | Default Value\n------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | -------------------------\nlauncher-sessions-enabled | Enables launching of rsession processes via the Job Launcher. This must be enabled to use the Job Launcher. | **N** | 0\nlauncher-address | TCP host/IP of the launcher host, or unix domain socket path (must match `/etc/rstudio/launcher.conf` configuration value). If using the default launcher configuration that ships with RStudio, this should be `localhost` (assuming you run the launcher side-by-side with RStudio Workbench). | **Y** |\nlauncher-port | Port that the launcher is listening on. Only required if not using unix domain sockets. If using the default launcher configuration that ships with RStudio, this should be `5559`. | **Y** |\nlauncher-default-cluster | Name of the cluster to use when launching sessions. Can be overridden by the launching user. | **N** |\nlauncher-sessions-callback-address | Address (HTTP or HTTPS) of RStudio Workbench that will be used by launcher sessions to communicate back for project sharing and launcher features. The address must be the reachable address of the `rserver` process from the host that will be running `rsession`, which in the case of launcher sessions can be on a different network segment entirely. If RStudio is configured to use SSL, you must also ensure that the callback address hostname matches the FQDN of the Common Name or one of the Subject Alternate Names on the HTTPS certificate. See the example configuration below for more details. | **Y** |\nlauncher-sessions-callback-verify-ssl-certs | Whether or not to verify SSL certificates when Launcher sessions are connecting to RStudio. Only applicable if connecting over HTTPS. For production use, you should always leave the default or have this set to true, but it can be disabled for testing purposes. | **N** | 1 |\nlauncher-use-ssl | Whether or not to connect to the launcher over HTTPS. Only supported for connections that do not use unix domain sockets. | **N** | 0\nlauncher-verify-ssl-certs  | Whether or not to verify SSL certificates when connecting to the launcher. Only applicable if connecting over HTTPS. For production use, you should always leave the default or have this set to true, but it can be disabled for testing purposes. | **N** | 1 |\nlauncher-sessions-clusters | Whitelist of clusters to allow for submitting interactive session jobs to. The default allows all job launcher clusters to run interactive sessions. | **N** |\nlauncher-adhoc-clusters | Whitelist of clusters to allow for submitting adhoc jobs from the Launcher pane. The default allows all job launcher clusters to run adhoc jobs. | **N** |\nlauncher-sessions-container-image | The default container image to use when creating sessions. Only required if using a plugin that requires containerization. If none is specified, the Job launcher-specified default will be used, if the plugin supports it. | **N** |\nlauncher-sessions-container-images | Comma-separated list of images which may be used for launching sessions. Used to filter out incompatible entries from the UI when a user is selecting an image to use for running the session. Leave blank to allow all images to be used. | **N** |\nlauncher-adhoc-container-images | Comma-separated list of images which may be used for launching adhoc jobs. Used to filter out incompatible entries from the UI when a user is selecting an image to use for running an adhoc job. Leave blank to allow all images to be used. | **N** |\nlauncher-sessions-container-run-as-root | Whether or not to run as root within the session container. We recommend you do not use this in most cases. | **N** | 0\nlauncher-sessions-create-container-user | Whether or not to create the session user within the container. Only applicable if using container sessions and not running containers as root. The created user will have the same UID, GID, home directory, and login shell as the user that launched the session. It is recommended that this option be used, unless your containers connect to an LDAP service to manage users and groups. The container starts as root so it can create the correct user and group ids, then drops privilege to use the created user account. If it cannot drop privilege the container will fail to start. | **N** | 1\nlauncher-sessions-forward-container-environment | Whether or not to forward any container environment variables to the session. This is useful for example, propogating Kubernetes secrets to the sesion. However, the variables `USER`, `HOME`, and `LOGNAME` are not forwarded, and are loaded from the user's passwd entry. | **N** | 1\nlauncher-sessions-connection-timeout-seconds | Number of seconds to allow for making the initial connection to a launcher session. Connection failures are retried automatically - this is simply to prevent unreachable hosts from hanging the retry process as the default connection timeout on most systems is very high. Only change this if you are having trouble connecting to sessions. A value of 0 indicates that there should be no timeout (system default). | **N** | 3\nlauncher-sessions-container-forward-groups | Whether or not to forward the user's supplemental groups to the created containers. This will only be done when not creating the container user, and when running the container as a non-root user, such as if integrating with LDAP. This is enabled by default, but if group lookups are very expensive in your environment and supplemental groups are not necessary, this can be disabled. | **N** | 1\n\nFor example, your `rserver.conf` file might look like the following:\n\n**/etc/rstudio/rserver.conf**\n\n```ini\nlauncher-address=localhost\nlauncher-port=5559\nlauncher-sessions-enabled=1\nlauncher-default-cluster=Kubernetes\n\n# the callback address that launcher sessions will reconnect to rserver on\n# since our Kubernetes jobs run on a different network segment, this needs\n# to be the routable IP address of the web server servicing RStudio traffic\n# (routable from the point of view of any Kubernetes nodes)\nlauncher-sessions-callback-address=http://10.15.44.30:8787\n\nlauncher-use-ssl=1\nlauncher-sessions-container-image=rstudio:R-3.5\nlauncher-sessions-container-run-as-root=0\nlauncher-sessions-create-container-user=1\n```\n\n#### SSL Considerations\n\nBoth RStudio Workbench and the Job Launcher can be configured to use SSL. When the Launcher is configured to use SSL, the RStudio Workbench node(s) that are connecting to the Launcher must ensure that the hostname configured in the `launcher-address` field matches the FQDN of the Common Name or Subject Alternate Name of the certificate that is presented by the Launcher. If the hostnames do not match exactly, SSL verification will fail, and RStudio will be unable to connect to the Job Launcher.\n\nSimilarly, if RStudio Workbench is configured to use SSL, the hostname configured in the `launcher-sessions-callback-address` field must match the FQDN of the Common Name or Subject Alternate Name of the certificate that is presented by RStudio. Failure to do so will cause certificate verification to fail when sessions attempt to connect to RStudio, preventing you from using Job Launcher functionality such as starting Launcher jobs.\n\nAdditionally, both the RStudio Workbench and Job Launcher root certificates need to be imported into the trusted root certificate store on the systems that are accessing those addresses. For example, the Workbench server nodes need to have the Job Launcher root certificate installed in their trusted certificate store to ensure that certificate verification works correctly. The exact steps for importing a certificate into the trusted root store are operating system specific and outside of the scope of this document.\n\n#### Job Launcher and PAM Sessions\n\nPAM Sessions work slightly differently when used with Launcher sessions. See [PAM Sessions with the Job Launcher] for more information.\n\n#### Authentication\n\nRStudio Workbench authenticates with the Job Launcher via the `secure-cookie-key` file, a secret key that is read on startup of both the launcher and RStudio which is only readable by the root account. The file is present at `/etc/rstudio/secure-cookie-key`. If the Job Launcher is running on a different machine than RStudio Workbench, you will need to make sure that the exact same `secure-cookie-key` file is present on both machines.\n\nTo do this, create a secure cookie key file on one of the nodes like so:\n\n```\n# generate secure-cookie-key as a simple UUID\nsudo sh -c \"echo `uuid` > /etc/rstudio/secure-cookie-key\"\n\n# ensure that the cookie is only readable by root\nsudo chmod 0600 /etc/rstudio/secure-cookie-key\n```\n\nOnce this file has been created, copy it to the other node to the same location so that both services use the same key. Alternatively, you could accomplish this via a symlink to a location on a file share.\n\nThe path to the `secure-cookie-key` file can be changed, but it is not recommended in most cases. If you need to change it, it can be done by adding the following line to the `/etc/rstudio/rserver.conf` and `/etc/rstudio/launcher.conf` configuration files:\n\n**/etc/rstudio/rserver.conf** and **/etc/rstudio/launcher.conf**\n\n```\nsecure-cookie-key-file=/path/to/secure-cookie-key\n```\n\nWhen running Launcher sessions in a load balanced RStudio deployment, sessions do additional authorization verification to ensure that they are only used by the user that created them. This is accomplished by an RSA key pair, located at `/etc/rstudio/launcher.pem` and `/etc/rstudio/launcher.pub`. These files must be the same on every RStudio node, or users will be unable to use their sessions on multiple nodes.\n\nIn order to create the RSA files, run the following commands:\n\n```\nsudo openssl genpkey -algorithm RSA -out /etc/rstudio/launcher.pem -pkeyopt rsa_keygen_bits:2048\nsudo openssl rsa -in /etc/rstudio/launcher.pem -pubout > /etc/rstudio/launcher.pub\nsudo chmod 0600 /etc/rstudio/launcher.pem\"\n```\n\nYou must ensure that the above private key (.pem) file is owned by root and has `600` permissions, as it *must* remain secret to your users.\n\nOnce the files are created, simply copy them to each RStudio node in your cluster.\n\n#### Launcher Sessions\n\nIt is recommended that you configure the Shared Storage path (see [Shared Storage] for configuration) in a location that will be reachable both by the RStudio Workbench instance and each Launcher Session in order to support various RStudio features. Failure to do so could cause subtle, unintended issues.\n\nSee the [Launcher Mounts] section for more details about how to configure this correctly with [Containerized Sessions].\n\n#### Containerized Sessions\n\nIn order to run your R sessions in containers, you will need a Docker image that contains the necessary `rsession` binaries installed. RStudio provides an official image for this purpose, which you can get from [Docker Hub](https://hub.docker.com/r/rstudio/r-session-complete).\n\nFor example, to get the RHEL6 image, you would run:\n\n```\ndocker pull rstudio/r-session-complete:centos7\n```\n\nAfter pulling the desired image, you will need to create your own Dockerfile that extends from the r-session-complete image and adds whatever versions of R you want to be available to your users, as well as adding any R packages that they will need. For example, your Dockerfile should look similar to the following:\n\n```\nFROM rstudio/r-session-complete:centos7\n\n# install desired versions of R\nRUN yum install -y R\n\n# install R packages\n...\n```\n\nSee [Docker Hub](https://hub.docker.com/r/rstudio/r-session-complete) for more information.\n\n##### Launcher Mounts\n\nWhen creating containerized sessions via the Job Launcher, you will need to specify mount points as appropriate to mount the users' home drives and any other desired paths. In order for sessions to run properly within containers, it is **required** to mount the home directories into the containers, as well as any directories containing per-user state (e.g., a customized `XDG_DATA_HOME`). The home mount path within the container must be the same as the user's home path as seen by the RStudio Workbench instance itself (generally, `/home/{USER}`).\n\nTo specify mount points, modify the `/etc/rstudio/launcher-mounts` file to consist of multiple mount entries separated by a blank line. The following table lists the fields that are available for each mount entry in the file.\n\nField | Description | Required (Y/N) | Default Value\n---------------- | -------------------------------------------------- | --- | --------\nMountType | The type of mount. Can be `Host`, `NFS`, `CephFs`, `GlusterFs`, `AzureFile`, `KubernetesPersistentVolumeClaim`, or `Passthrough` | **Y**\nMountPath | The path within the container that the directory will be mounted to. | **Y** |\nReadOnly | Whether or not the mount is read only. Can be true or false. | **N** | false\nJobType | What type of jobs the mount is applied to. Can be `session`, `adhoc`, or `any`. | **N** | any\nWorkbench | What type of workbench the mount is applied to. Can be `rstudio`, `jupyterlab`, `jupyter notebook`, `vs code`, or `any`. | **N** | any\nCluster | The specific cluster that this mount applies to. Applies to all clusters if not specified. | **N** |\n\nDepending on the `MountType` specified above, different settings may be used to control the mount.\n\n**MountType: Host**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nPath | The source directory of the mount, i.e. where the mount data comes from. | **Y**\n\n**MountType: NFS**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nPath | The source directory of the mount, i.e. where the mount data comes from. | **Y**\nHost | The NFS host name for the NFS mount. | **N**\n\n**MountType: CephFs**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nMonitors | A comma-separated list of Ceph monitor addresses. For example: `192.168.1.200:8765,192.168.1.200:8766` | **Y**\nPath | The path within the Ceph filesystem to mount | **N**\nUser | The Ceph username to use | **N**\nSecretFile | The file which contains the Ceph keyring for authentication | **N**\nSecretRef | Reference to Ceph authentication secrets, which overrides `SecretFile` if specified | **N**\n\n**MountType: GlusterFs**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nEndpoints | The name of the endpoints object that represents a Gluster cluster configuration | **Y**\nPath | The name of the GlusterFs volume | **Y**\n\n**MountType: AzureFile**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nSecretName | The name of the secret that contains both the Azure storage account name and the key | **Y**\nShareName | The share name to be used\n\n**MountType: KubernetesPersistentVolumeClaim**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nClaimName | The name of the Kubernetes Persistent Volume Claim to use | **Y**\n\n**MountType: Passthrough**\n\nField | Description | Required (Y/N)\n------ | ---------------------------------------------- | ----\nFilePath | Path to a file that contains the raw JSON object representing the mount, which is sent directly to the back-end without transformation | **Y**\n\nNote that for many mount types, paths may contain the special variable `{USER}` to indicate that the user's name be substituted, enabling you to mount user-specific paths.\n\nAn example `/etc/rstudio/launcher-mounts` file is shown below.\n\n*/etc/rstudio/launcher-mounts*\n\n```ini\n# User home mount - This is REQUIRED for the session to run\nMountType: NFS\nHost: nfs01\nPath: /home/{USER}\nMountPath: /home/{USER}\nReadOnly: false\n\n# Shared code mount\nCluster: Kubernetes\nMountType: NFS\nHost: nfs01\nPath: /dev64\nMountPath: /code\nReadOnly: false\n\n# Only mount the following directory when the user is launching a JupyterLab session\nCluster: Kubernetes\nWorkbench: JupyterLab\nMountType: CephFs\nMonitors: 127.0.0.1:8080,127.0.0.1:8081\nSecretFile: /etc/secrets/ceph\nReadOnly: true\n```\n\nIt is important that each entry consists of the fields as specified above. Each field must go on its own line. There should be no empty lines between field definitions. Each entry must be separated by one full blank line (two new-line `\\n` characters).\n\nIf you choose to run your containers as root, the user home drive **must** be mapped to `/root`. For example:\n\n*/etc/rstudio/launcher-mounts*\n\n```ini\nMountType: NFS\nHost: nfs01\nPath: /home/{USER}\nMountPath: /root\nReadOnly: false\n```\n\nAs noted in the [Launcher Sessions] section, it is recommended that you also mount the Shared Storage path (see [Shared Storage] for configuration) into the session container to support various RStudio features. When mounting the shared storage path, ensure that the folder is mounted to the same path within the container to ensure that the `rsession` executable will correctly find it. For example:\n\n*/etc/rstudio/launcher-mounts*\n\n```ini\nMountType: NFS\nHost: nfs01\nPath: /rstudio/shared-storage\nMountPath: /rstudio/shared-storage\nReadOnly: false\n```\n\n##### Launcher Environment\n\nYou may optionally specify environment variables to set when creating launcher sessions.\n\nTo specify environment overrides, modify the `/etc/rstudio/launcher-env` file to consist of multiple environment entries separated by a blank line. The following table lists the fields that are available for each environment entry in the file.\n\nField | Description | Required (Y/N) | Default Value\n---------------- | -------------------------------------------------- | --- | --------\nJobType | What type of jobs the environment value(s) is applied to. Can be `session`, `adhoc`, or `any`. | **N** | any\nWorkbench | What type of workbench the mount is applied to. Can be `rstudio`, `jupyterlab`, `jupyter notebook`, `vs code`, or `any`. | **N** | any\nCluster | The specific cluster that the environment applies to. Applies to all clusters if not specified. | **N** |\nEnvironment | The environment variables to set, one per line (each subsequent line must be indented with an arbitrary amount of spaces or tabs), in the form of `KEY=VALUE` pairs. | **N** |\n\nAdditionally, you can use the special `{USER}` variable to specify the value of the launching user's username, similar to the mounts file above.\n\nAn example `/etc/rstudio/launcher-env` file is shown below.\n\n*/etc/rstudio/launcher-env*\n\n```ini\nJobType: session\nEnvironment: IS_LAUNCHER_SESSION=1\n IS_ADHOC_JOB=0\n USER_HOME=/home/{USER}\n\nJobType: adhoc\nEnvironment: IS_LAUNCHER_SESSION=0\n IS_ADHOC_JOB=1\n USER_HOME=/home/{USER}\n\nJobType: any\nCluster: Kubernetes\nENVIRONMENT: IS_KUBERNETES=1\n```\n\nIf you do not need to set different environment variables for different job types or different clusters, you may simply specify `KEY=VALUE` pairs, one per line, which will be applied to all launcher ad-hoc jobs and sessions. For example:\n\n```ini\nIS_LAUNCHER_JOB=1\nUSER_HOME=/home/{USER}\n```\n\n##### Launcher Ports\n\nYou may optionally specify ports that should be exposed when creating containerized jobs. This will allow the ports to be exposed within the host running the container, allowing the ports to be reachable from external services. For example, for Shiny applications to be usable, you must expose the desired Shiny port, otherwise the browser window will not be able to connect to the Shiny application running within the container.\n\nTo specify exposed ports, modify the `/etc/rstudio/launcher-ports` file to consist of multiple port entries separated by a blank line. The following table lists the fields that are available for each port entry in the file.\n\nField | Description | Required (Y/N) | Default Value\n---------------- | -------------------------------------------------- | --- | --------\nJobType | What type of jobs the port(s) is applied to. Can be `session`, `adhoc`, or `any`. | **N** | any\nWorkbench | What type of workbench the mount is applied to. Can be `rstudio`, `jupyterlab`, `jupyter notebook`, `vs code`, or `any`. | **N** | any\nCluster | The specific cluster that this set of ports applies to. Applies to all clusters if not specified. | **N** |\nPorts | The ports to expose, one per line (each subsequent line must be indented with an arbitrary amount of spaces or tabs). | **N** |\n\nAn example `/etc/rstudio/launcher-ports` file is shown below.\n\n*/etc/rstudio/launcher-ports*\n\n```ini\nJobType: adhoc\nPorts: 6210\n 6143\n 6244\n 6676\n\n# additional Kubernetes ports to expose\nJobType: adhoc\nCluster: Kubernetes\nPorts: 4434\n```\n\nIf you do not need to set different exposed ports for different job types or different clusters, you may simply specify port values, one per line, which will be applied to all launcher ad-hoc jobs and sessions. For example:\n\n*/etc/rstudio/launcher-ports*\n\n```ini\n5873\n5874\n64234\n64235\n```\n\n#### Containerized Adhoc Jobs\n\nTo run adhoc jobs in containers from the Launcher pane, you need a Docker image containing the bash shell and the desired version of R on the path.\n\nThe adhoc job container will run using the same userId and groupId value as the RStudio user. In order for scripts under the home directory to be found in the container, the home directory must be mounted with the same absolute path inside the container.\n\nJobs started from the RStudio console via `rstudioapi::launcherSubmitJob()` have no specific container requirements.\n\n## Running the Launcher\n\nOnce it is configured, you can run the Job Launcher by invoking the command `sudo rstudio-launcher start`, and stop it with `sudo rstudio-launcher stop`. The Job Launcher must be run with root privileges, but similar to `rstudio-server`, privileges are immediately lowered. Root privileges are used only to impersonate users as necessary.\n\n## Load Balancing\n\nBoth RStudio Workbench and the Job Launcher services can be load balanced, providing maximum scalability and redundancy. When using the RStudio Workbench load balancer with the Launcher, it is generally sufficient to simply have each Workbench node point to its own node-local Launcher service via `rserver.conf` configuration - no external load balancer needs to control access to the Launcher itself.\n\n::: {.callout-note}\nIn this mode, when using the local Launcher, sessions will be balanced according to the setting you have defined under `balancer` in */etc/rstudio/load-balancer*.\n:::\n\nHowever, in some cases, you may want to scale the Job Launcher separately from RStudio Workbench. For example, if your Launcher cluster needs to exist in a different network for security reasons, such as to limit node connectivity to backend services (e.g., Kubernetes). In such cases, you will need to scale the Job Launcher separately via an external load balancer, and Workbench should be configured to point to this load balanced instance of the Job Launcher. In most cases, the external load balancer should be configured for sticky sessions, which will ensure that each instance of Workbench connects to just one Job Launcher node, providing the most consistent view of the current job state. For more information on configuring the Job Launcher for load balancing, see the [Job Launcher documentation](https://docs.rstudio.com/job-launcher/99.9.9/index.html).\n\nIt should be noted that in most cases, load balancing is not needed for performance reasons, and is generally used for redundancy purposes.\n\n## Creating plugins\n\nPlugins allow communication with specific batch cluster / container orchestration systems like Slurm and Kubernetes. However, you may be using a system that RStudio does not natively support. The [RStudio Launcher Plugin SDK](https://rstudio.com/products/launcher-plugin-sdk) can be used to quickly develop Plugins in C/C++.\n\n## Troubleshooting {#launcher-troubleshooting}\n\nIf you experience issues related to running Launcher sessions, adhoc jobs, Jupyter sessions, or VS Code sessions, you can use the Launcher verification tool which will attempt to launch jobs and provide diagnostic output about what could be going wrong. To run the verification process, run the following command:\n\n```ini\nsudo rstudio-server verify-installation --verify-user=user\n```\n\nReplace the `--verify-user` value with a valid username of a user that is setup to run RStudio Workbench in your installation. This will cause the test jobs to be started under their account, allowing the verification tool to check additional aspects of launching jobs, including mounting the user's home directories into containers. You can also specify a specific test to run by using the `--verify-test` flag, like so:\n\n```ini\nsudo rstudio-server verify-installation --verify-user=user --verify-test=r-sessions\n```\n\nThe above example will only test R Sessions, skipping adhoc jobs and Jupyter/VS Code sessions. The parameter can be one of `r-sessions`, `adhoc-jobs`, `jupyter-sessions`, or `vscode-sessions`. If the parameter is unspecified, all tests will be run.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "dependencies": {
      "type": "includes",
      "data": []
    },
    "preserve": {},
    "postProcess": null
  }
}